{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0616098 黃秉茂 ML HW4 SVM and ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id      cuisine                                        ingredients\n",
      "0      22675      italian  [1% low-fat cottage cheese, low-fat marinara s...\n",
      "1      32288  southern_us  [brown sugar, salt, eggs, butter, chopped peca...\n",
      "2      44406         thai  [red chili peppers, bell pepper, garlic, fish ...\n",
      "3      29355     moroccan  [water, green tea leaves, tangerine, fresh min...\n",
      "4      39350      chinese  [vegetable oil, chile sauce, tomato paste, gar...\n",
      "...      ...          ...                                                ...\n",
      "29769   2278     japanese  [soy sauce, sesame oil, garlic, sake, flour, g...\n",
      "29770    474   vietnamese  [mint, garlic sauce, chinese chives, rice nood...\n",
      "29771  44229       indian  [potatoes, vegetable broth, oil, cashew nuts, ...\n",
      "29772  20311  southern_us  [butter, powdered sugar, cream cheese, soften,...\n",
      "29773  32823      chinese  [savoy cabbage, dumpling wrappers, ginger, soy...\n",
      "\n",
      "[29774 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=dataset.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_last_word = dataset.copy()\n",
    "dataset_last_2_word = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limit the number of words in a string which is in the list in a column using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_limit = 1\n",
    "dataset_last_word[dataset_last_word.columns[-1]] = dataset_last_word[dataset_last_word.columns[-1]].apply(lambda x : [' '.join(item.split()[-word_limit:]) for item in x])\n",
    "word_limit = 2\n",
    "dataset_last_2_word[dataset_last_2_word.columns[-1]] = dataset_last_2_word[dataset_last_2_word.columns[-1]].apply(lambda x : [' '.join(item.split()[-word_limit:]) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cuisine                                        ingredients\n",
      "0          italian  [cheese, sauce, starch, spinach, whites, nutme...\n",
      "1      southern_us  [sugar, salt, eggs, butter, pecans, halves, ex...\n",
      "2             thai  [peppers, pepper, garlic, sauce, sauce, ginger...\n",
      "3         moroccan  [water, leaves, tangerine, mint, sugar, water,...\n",
      "4          chinese  [oil, sauce, paste, cloves, cabbage, vinegar, ...\n",
      "...            ...                                                ...\n",
      "29769     japanese  [sauce, oil, garlic, sake, flour, pork, wrappe...\n",
      "29770   vietnamese  [mint, sauce, chives, noodles, shrimp, lettuce...\n",
      "29771       indian  [potatoes, broth, oil, nuts, pepper, paprika, ...\n",
      "29772  southern_us                   [butter, sugar, soften, extract]\n",
      "29773      chinese  [cabbage, wrappers, ginger, sauce, oil, shrimp...\n",
      "\n",
      "[29774 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_last_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffle = shuffle(dataset)\n",
    "dataset_last_word_shuffle = shuffle(dataset_last_word)\n",
    "dataset_last_2_word_shuffle = shuffle(dataset_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_shuffle[dataset_shuffle.columns[-1]]\n",
    "x_last_word = dataset_last_word_shuffle[dataset_last_word_shuffle.columns[-1]]\n",
    "x_last_2_word = dataset_last_2_word_shuffle[dataset_last_2_word_shuffle.columns[-1]]\n",
    "y = dataset_shuffle[dataset_shuffle.columns[0]]\n",
    "y_last_word = dataset_last_word_shuffle[dataset_last_word_shuffle.columns[0]]\n",
    "y_last_2_word = dataset_last_2_word_shuffle[dataset_last_2_word_shuffle.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encode a series of lists in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "X = pd.DataFrame(mlb.fit_transform(x), columns=mlb.classes_, index=x.index)\n",
    "X_last_word = pd.DataFrame(mlb.fit_transform(x_last_word), columns=mlb.classes_, index=x_last_word.index)\n",
    "X_last_2_word = pd.DataFrame(mlb.fit_transform(x_last_2_word), columns=mlb.classes_, index=x_last_2_word.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X_last_word = np.array(X_last_word)\n",
    "X_last_2_word = np.array(X_last_2_word)\n",
    "Y = np.array(y)\n",
    "Y_last_word = np.array(y_last_word)\n",
    "Y_last_2_word = np.array(y_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29774, 6231)\n",
      "(29774, 1764)\n",
      "(29774, 4909)\n"
     ]
    }
   ],
   "source": [
    "print('no word limit', X.shape)\n",
    "print('last word', X_last_word.shape)\n",
    "print('last 2 word', X_last_2_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_with_proper_n_com(data):\n",
    "    n_com = 9\n",
    "    explained_ratio = 0\n",
    "    explained_ratio_threshold = 80\n",
    "    while explained_ratio < explained_ratio_threshold:\n",
    "        n_com += 1\n",
    "        pca = PCA(n_components=n_com)\n",
    "        # calculate variance ratios\n",
    "        pca.fit(data)\n",
    "        cumulative_sum_of_variance_explained = np.cumsum(pca.explained_variance_ratio_ * 100)\n",
    "        explained_ratio = cumulative_sum_of_variance_explained[-1]\n",
    "    pca = PCA(n_components = n_com)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    return data_pca, n_com, explained_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_com: 508 explained variance ratio: 80.02134297358639\n",
      "n_com: 110 explained variance ratio: 80.03405704066961\n",
      "n_com: 385 explained variance ratio: 80.00841017452635\n"
     ]
    }
   ],
   "source": [
    "X_pca, X_proper_n_com, X_explained_ratio = pca_with_proper_n_com(X)\n",
    "X_pca_last_word, X_proper_n_com_last_word, X_explained_ratio_last_word = pca_with_proper_n_com(X_last_word)\n",
    "X_pca_last_2_word, X_proper_n_com_last_2_word, X_explained_ratio_last_2_word = pca_with_proper_n_com(X_last_2_word)\n",
    "print('n_com:', X_proper_n_com, 'explained variance ratio:', X_explained_ratio)\n",
    "print('n_com:', X_proper_n_com_last_word, 'explained variance ratio:', X_explained_ratio_last_word)\n",
    "print('n_com:', X_proper_n_com_last_2_word, 'explained variance ratio:', X_explained_ratio_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29774, 508)\n",
      "(29774, 110)\n",
      "(29774, 385)\n",
      "(29774, 6231)\n"
     ]
    }
   ],
   "source": [
    "print('no word limit', X_pca.shape)\n",
    "print('last word', X_pca_last_word.shape)\n",
    "print('last 2 word', X_pca_last_2_word.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_model = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear_model = SVC(kernel = 'linear')\n",
    "svc_poly_model = SVC(kernel = 'poly')\n",
    "svc_rbf_model = SVC(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "x_train_last_word, x_val_last_word, y_train_last_word, y_val_last_word = train_test_split(X_last_word, Y_last_word, test_size=0.3, random_state=0)\n",
    "x_train_last_2_word, x_val_last_2_word, y_train_last_2_word, y_val_last_2_word = train_test_split(X_last_2_word, Y_last_2_word, test_size=0.3, random_state=0)\n",
    "\n",
    "x_train_pca, x_val_pca, y_train_pca, y_val_pca = train_test_split(X_pca, Y, test_size=0.3, random_state=0)\n",
    "x_train_pca_last_word, x_val_pca_last_word, y_train_pca_last_word, y_val_pca_last_word = train_test_split(X_pca_last_word, Y_last_word, test_size=0.3, random_state=0)\n",
    "x_train_pca_last_2_word, x_val_pca_last_2_word, y_train_pca_last_2_word, y_val_pca_last_2_word = train_test_split(X_pca_last_2_word, Y_last_2_word, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = linear_svc_model.fit(x_train, y_train)\n",
    "pred_linear_svc = linear_svc.predict(x_val)\n",
    "\n",
    "linear_svc_last_word = linear_svc_model.fit(x_train_last_word, y_train_last_word)\n",
    "pred_linear_svc_last_word = linear_svc_last_word.predict(x_val_last_word)\n",
    "\n",
    "linear_svc_last_2_word = linear_svc_model.fit(x_train_last_2_word, y_train_last_2_word)\n",
    "pred_linear_svc_last_2_word = linear_svc_last_2_word.predict(x_val_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear = svc_linear_model.fit(x_train_pca, y_train_pca)\n",
    "pred_svc_linear = svc_linear.predict(x_val_pca)\n",
    "\n",
    "svc_linear_last_word = svc_linear_model.fit(x_train_pca_last_word, y_train_pca_last_word)\n",
    "pred_svc_linear_last_word = svc_linear_last_word.predict(x_val_pca_last_word)\n",
    "\n",
    "svc_linear_last_2_word = svc_linear_model.fit(x_train_pca_last_2_word, y_train_pca_last_2_word)\n",
    "pred_svc_linear_last_2_word = svc_linear_last_2_word.predict(x_val_pca_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly = svc_poly_model.fit(x_train_pca, y_train_pca)\n",
    "pred_svc_poly = svc_poly.predict(x_val_pca)\n",
    "\n",
    "svc_poly_last_word = svc_poly_model.fit(x_train_pca_last_word, y_train_pca_last_word)\n",
    "pred_svc_poly_last_word = svc_poly_last_word.predict(x_val_pca_last_word)\n",
    "\n",
    "svc_poly_last_2_word = svc_poly_model.fit(x_train_pca_last_2_word, y_train_pca_last_2_word)\n",
    "pred_svc_poly_last_2_word = svc_poly.predict(x_val_pca_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_rbf = svc_rbf_model.fit(x_train_pca, y_train_pca)\n",
    "pred_svc_rbf = svc_rbf.predict(x_val_pca)\n",
    "\n",
    "svc_rbf_last_word = svc_rbf_model.fit(x_train_pca_last_word, y_train_pca_last_word)\n",
    "pred_svc_rbf_last_word = svc_rbf.predict(x_val_pca_last_word)\n",
    "\n",
    "svc_rbf_last_2_word = svc_rbf_model.fit(x_train_pca_last_2_word, y_train_pca_last_2_word)\n",
    "pred_svc_rbf_last_2_word = svc_rbf_last_2_word.predict(x_val_pca_last_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no word limit\n",
      "linear_svc 0.7522668756296876\n",
      "svc_linear 0.7056979738049928\n",
      "svc_poly 0.5780812716892422\n",
      "svc_rbf 0.7144296428971231\n",
      "last word\n",
      "linear_svc 0.7101757528265981\n",
      "svc_linear 0.6495018470838464\n",
      "svc_poly 0.637299899249972\n",
      "svc_rbf 0.6763685212134781\n",
      "last 2 word\n",
      "linear_svc 0.7690585469607075\n",
      "svc_linear 0.7204746445762902\n",
      "svc_poly 0.616142393372887\n",
      "svc_rbf 0.7276390910108587\n"
     ]
    }
   ],
   "source": [
    "print('no word limit')\n",
    "print('linear_svc', accuracy_score(y_val, pred_linear_svc))\n",
    "print('svc_linear', accuracy_score(y_val_pca, pred_svc_linear))\n",
    "print('svc_poly', accuracy_score(y_val_pca, pred_svc_poly))\n",
    "print('svc_rbf', accuracy_score(y_val_pca, pred_svc_rbf))\n",
    "\n",
    "print('last word')\n",
    "print('linear_svc', accuracy_score(y_val_last_word, pred_linear_svc_last_word))\n",
    "print('svc_linear', accuracy_score(y_val_pca_last_word, pred_svc_linear_last_word))\n",
    "print('svc_poly', accuracy_score(y_val_pca_last_word, pred_svc_poly_last_word))\n",
    "print('svc_rbf', accuracy_score(y_val_pca_last_word, pred_svc_rbf_last_word))\n",
    "\n",
    "print('last 2 word')\n",
    "print('linear_svc', accuracy_score(y_val_last_2_word, pred_linear_svc_last_2_word))\n",
    "print('svc_linear', accuracy_score(y_val_pca_last_2_word, pred_svc_linear_last_2_word))\n",
    "print('svc_poly', accuracy_score(y_val_pca_last_2_word, pred_svc_poly_last_2_word))\n",
    "print('svc_rbf', accuracy_score(y_val_pca_last_2_word, pred_svc_rbf_last_2_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn中 Decision Tree 使用的演算法是CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 普遍上Cross-Validation的表現比Holdout Validation好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
